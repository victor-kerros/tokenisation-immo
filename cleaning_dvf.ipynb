{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660234b5",
   "metadata": {},
   "source": [
    "# *Projet Python ENSAE*\n",
    "\n",
    "## Cleanning DVF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4b487",
   "metadata": {},
   "source": [
    "Ce notebook a pour but de **cleaner le dataset DVF** conformément au **problème de preprocessing** que nous avons identifié.\n",
    "\n",
    "Dans le notebook principal \"valo-immo\", **par souci d'efficacité**, nous avons retenu **la solution alternative** qui consiste à ne retenir que les transactions n'ayant pas fait l'objet d'une duplication de ligne.\n",
    "\n",
    "En effet, la solution de nettoyage que nous avons trouvée étant **très coûteuse en temps**, nous nous permettons de la proposer ici. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154eea4",
   "metadata": {},
   "source": [
    "# Etape 0 : packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38547454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tdqm\n",
      "  Downloading tdqm-0.0.1.tar.gz (1.4 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tqdm in c:\\users\\leo\\anaconda3\\lib\\site-packages (from tdqm) (4.59.0)\n",
      "Building wheels for collected packages: tdqm\n",
      "  Building wheel for tdqm (setup.py): started\n",
      "  Building wheel for tdqm (setup.py): finished with status 'done'\n",
      "  Created wheel for tdqm: filename=tdqm-0.0.1-py3-none-any.whl size=1322 sha256=2af9074575eb36244b4c245598fae23c7cbadcf104e50e7b78dadefbcfeff586\n",
      "  Stored in directory: c:\\users\\leo\\appdata\\local\\pip\\cache\\wheels\\c6\\f0\\d9\\9fa5ff78c0f9d5a0a427bbbb4893c283520ddfccb885ea2205\n",
      "Successfully built tdqm\n",
      "Installing collected packages: tdqm\n",
      "Successfully installed tdqm-0.0.1\n"
     ]
    }
   ],
   "source": [
    "# importation des packages importants\n",
    "\n",
    "!pip install tdqm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97025158",
   "metadata": {},
   "source": [
    "# Etape 1 : Importation de DVF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2c34c-fc10-4be2-b04e-961ad0f01601",
   "metadata": {},
   "source": [
    "Nous importons le dataset « **Demandes de valeurs foncières** » (DVF), publié par la DGFiP, permet de connaître les transactions immobilières intervenues au cours des cinq dernières années sur le territoire métropolitain et les DOM-TOM, à l’exception de l’Alsace, de la Moselle et de Mayotte. Les données contenues sont issues des actes notariés et des informations cadastrales.\n",
    "\n",
    "Fichiers 2017-2020 : https://files.data.gouv.fr/geo-dvf/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c30121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leo\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3166: DtypeWarning: Columns (10,12,17,18,20,22,24,35,36) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 166. MiB for an array with shape (18, 1210569) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7fdddd011b8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://files.data.gouv.fr/geo-dvf/latest/csv/2021/full.csv.gz\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# On ne travaille que sur les données du S1 2021, afin d'avoir des calculs moins coûteux en temps.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1067\u001b[0m             \u001b[0mnew_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1070\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_currow\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnew_rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         ]\n\u001b[1;32m--> 287\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   1699\u001b[0m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCPandasArray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1700\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1701\u001b[1;33m         \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_form_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1702\u001b[0m         \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1703\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_form_blocks\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   1757\u001b[0m     \u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1758\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"FloatBlock\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1759\u001b[1;33m         \u001b[0mfloat_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_multi_blockify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"FloatBlock\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1760\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_multi_blockify\u001b[1;34m(tuples, dtype)\u001b[0m\n\u001b[0;32m   1850\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup_block\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1852\u001b[1;33m         \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_stack_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup_block\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m         \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_stack_arrays\u001b[1;34m(tuples, dtype)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0m_shape_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1880\u001b[1;33m     \u001b[0mstacked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1881\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mstacked\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 166. MiB for an array with shape (18, 1210569) and data type float64"
     ]
    }
   ],
   "source": [
    "# Les fichiers sont issus de https://files.data.gouv.fr/geo-dvf/latest/\n",
    "\n",
    "name = \"https://files.data.gouv.fr/geo-dvf/latest/csv/2021/full.csv.gz\"\n",
    "table = pd.read_csv(name, sep = ',')\n",
    "\n",
    "# On ne travaille que sur les données du S1 2021, afin d'avoir des calculs moins coûteux en temps.\n",
    "# Les transactions du S1 2021 représentent tout de même un dataset de 1 200 000 lignes...\n",
    "# Pour travailler sur l'ensemble des données (2017-2021), il suffit d'enlever les guillemets ci-dessous.\n",
    "\n",
    "\"\"\"\n",
    "for year in range(2017, 2021):\n",
    "    name = \"https://files.data.gouv.fr/geo-dvf/latest/csv/\" + str(year) + \"/full.csv.gz\"\n",
    "    table = pd.concat([table, pd.read_csv(name, sep = ',')])\n",
    "\n",
    "display(\"Taille de table :\")\n",
    "display(table.shape)\n",
    "table.head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bbea74-f4ac-4d60-b193-13c7a770c74d",
   "metadata": {},
   "source": [
    "# Etape 2 : observation du problème de preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313f9ecc-fbac-477f-a3d0-fdc27d14e487",
   "metadata": {
    "tags": []
   },
   "source": [
    "On se rend compte du **problème de preprocessing** mentionné plus haut : une transaction correspond à plusieurs lignes.\n",
    "Ainsi, si on entraîne l'algorithme de pricing sur ce dataset, il sera **biaisé** : \n",
    "* d'une part, il associerait à une dépendance de 20 m2 le prix d'un appartement de 200 m2\n",
    "* d'autre part, il ne prendrait pas en compte la plus-value apportée par un jardin à une maison, par une dépendance à un appartement, etc.\n",
    "\n",
    "Il conviendra donc de **retravailler les données pour obtenir une seule ligne par transaction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9350dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une adresse générique\n",
    "\n",
    "table['adresse_numero'] = table['adresse_numero'].fillna('0').astype(int)\n",
    "table['adresse_suffixe'] = table['adresse_suffixe'].fillna(' ')\n",
    "table['adresse_code_voie'] = table['adresse_code_voie'].fillna(' ')\n",
    "table['adresse_nom_voie'] = table['adresse_nom_voie'].fillna(' ')\n",
    "table['code_postal'] = table['code_postal'].fillna('0').astype(int)\n",
    "table['nom_commune'] = table['nom_commune'].fillna(' ')\n",
    "\n",
    "#Ajout de \"\\\" pour que l'opération soit visible à l'écran en entier\n",
    "table[\"adresse\"] = table['adresse_numero'].astype(str) + ' ' + table['adresse_suffixe'] + ' ' + \\\n",
    "                table['adresse_code_voie'] + ' ' + table['adresse_nom_voie'] + ' ' + table['nom_commune'] + ' ' + \\\n",
    "                table['code_postal'].astype(str) + ' ' + 'France'\n",
    "\n",
    "# Création d'un identifiant de transaction\n",
    "# Pour identifier les doublons, l'adresse ne suffit pas : un bien peut avoir été vendu deux fois dans la même année\n",
    "table[\"identifiant_transaction\"] = table[\"adresse\"].astype(str) + ' le ' + table[\"date_mutation\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e7012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problème dans les données et vérification de la validité de l'identifiant de transaction :\n",
    "\n",
    "display(table[\"identifiant_transaction\"].loc[0])\n",
    "display(table[\"identifiant_transaction\"].loc[1])\n",
    "display(\"Si l'identifiant de transaction est valide, alors True doit s'afficher :\")\n",
    "table[\"identifiant_transaction\"].loc[0] == table[\"identifiant_transaction\"].loc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe98070-29e7-4521-8a8e-96585e09622e",
   "metadata": {},
   "source": [
    "**Problème de preprocessing** : le dataset affiche le même prix de vente global à chaque lot.\n",
    "\n",
    "En effet, d'après la **notice descriptive** : \n",
    "* Les lots, notion juridique immobilière, sont définis dans un état descriptif de division (EDD) et dans les documents de mutation. Ils permettent d’identifier une partie d’un immeuble et ainsi d’y associer un droit de propriété spécifique (propriété, usufruit, etc.).\n",
    "* Les locaux, notion fiscale, identifient les mêmes parties d’un immeuble pour les besoins de la taxe foncière et de la taxe d’habitation en regroupant plusieurs lots.\n",
    "* La correspondance entre le découpage en lots et en locaux n’est pas retracée.\n",
    "\n",
    "Donc, quand une disposition comporte plusieurs locaux ou plusieurs natures de culture, **le fichier de restitution comporte autant de lignes qu’il y a de locaux ou de nature de culture concernés par la mutation**. Ainsi, pour une même publication, il peut y avoir 1 à n ligne(s) de restitution. **Les données génériques (ainsi que le prix) sont alors répétées sur chaque ligne**.\n",
    "\n",
    "Voici les types de locaux différents : \n",
    "['Dépendance', 'Appartement', 'Maison', nan, 'Local industriel. commercial ou assimilé']\n",
    "On ne peut pas juste enlever les \"Dépendances\". En effet, ces dernières peuvent avoir un effet (haussier) sur le prix de vente.\n",
    "\n",
    "Par ailleurs, une maison avec un jardin fait l'objet de deux lignes ayant la même valeur foncière mais des surfaces différentes (celle de la maison, celle du jardin) : suivant la colonne nature_culture ([nan, 'sols', \"terrains d'agrément\", 'taillis simples', 'terrains a bâtir', 'eaux', 'landes', 'taillis sous futaie', 'prés', 'terres', 'jardins', 'peupleraies', 'vignes', 'bois', 'vergers', 'carrières', 'futaies résineuses', 'pâtures', 'futaies feuillues', 'futaies mixtes', 'chemin de fer', 'oseraies', 'pacages', 'prés plantes', 'terres plantées', 'landes boisées', 'herbages', \"prés d'embouche\"]).\n",
    "\n",
    "Il conviendra donc d'**agréger dans une ligne ces informations** :\n",
    "* l'actif immobilier a-t-il une dépendance ? si oui, de combien de m2 ?\n",
    "* la maison a-t-elle un jardin ? si oui, de combien de m2 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d6d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On constate, encore une fois, le problème relevé ci-dessus :\n",
    "\n",
    "display(\"Nombre d'adresses uniques dans le DataFrame :\")\n",
    "display(len(table[\"adresse\"].unique()))\n",
    "\n",
    "display(\"Nombre d'identifiant_transaction uniques dans le DataFrame :\")\n",
    "display(len(table[\"identifiant_transaction\"].unique()))\n",
    "\n",
    "display(\"Nombre de lignes dans le DataFrame :\")\n",
    "display(len(table))\n",
    "\n",
    "display(\"Nombre moyen de lignes par vente :\")\n",
    "np.round(len(table) / len(table[\"identifiant_transaction\"].unique()), 2)\n",
    "\n",
    "# Une vente correspond à plusieurs lignes, les informations sont donc diffusées dans ces lignes..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0112def2-822c-4e00-b348-37ea855ff03f",
   "metadata": {},
   "source": [
    "# Etape 3 : Création du dataset final DVF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a821580-1ce4-4a5e-ba8f-d4675ba32c3d",
   "metadata": {},
   "source": [
    "### 3.1. Choix des colonnes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73870b-6be9-46a3-8851-e3e92655159a",
   "metadata": {},
   "source": [
    "D'abord, nous ne prenons que les colonnes suivantes :\n",
    "- Date de vente/mutation\n",
    "- Nature mutation (pour séparer les ventes en VEFA et les ventes classiques)\n",
    "- Valeur foncière (prix de vente)\n",
    "- Colonnes liées à l’adresse (pour nous permettre de localiser le bien)\n",
    "- Adresse\n",
    "- Code Postal\n",
    "- Type local (maison/appartement/Local commercial/Dépendance etc)\n",
    "- Surface réelle bâtie (nb de mètre carré du bien bâti)\n",
    "- Surface terrain (nb de mètre carré du terrain associé au bien)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de table_vf :\n",
    "\n",
    "# On crée le dataframe table_vf qui sera la version finale (vf) de notre dataset\n",
    "colonnes = [\"date_mutation\", \"nature_mutation\", \"valeur_fonciere\", \"code_postal\", 'type_local',\n",
    "            'surface_reelle_bati', 'nombre_pieces_principales', 'nature_culture', 'surface_terrain', 'longitude', \n",
    "            'latitude', 'adresse', 'code_departement', 'identifiant_transaction']\n",
    "table_vf = table[colonnes].copy()\n",
    "\n",
    "# On agrège les types de cultures différents de NaN, sols, terrain à bâtir et  : on les renomme \"culture\"\n",
    "culture_type = ['taillis simples', 'eaux', 'landes', 'taillis sous futaie', 'prés', 'terres', 'peupleraies', \n",
    "                'vignes', 'bois', 'vergers', 'carrières', 'futaies résineuses', 'pâtures', 'futaies feuillues', \n",
    "                'futaies mixtes', 'chemin de fer', 'oseraies', 'pacages', 'prés plantes', 'terres plantées', \n",
    "                'landes boisées', 'herbages', \"prés d'embouche\"]\n",
    "\n",
    "for x in culture_type:\n",
    "    table_vf.loc[table_vf[\"nature_culture\"] == x, \"nature_culture\"] = \"culture\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1798b71c-26a5-49ed-b848-129ab0da7ad3",
   "metadata": {},
   "source": [
    "### 3.2. Création de deux dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5119c9-a237-4d10-81e9-301351c2f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée deux dataframes :\n",
    "# * un dataframe avec les transactions non dupliquées\n",
    "# * un dataframe avec les transactions dupliquées (sur lequel il faudra travailler pour parvenir à une transaction = une ligne)\n",
    "\n",
    "table_vf_dup = table_vf.copy()\n",
    "table_vf_uni = table_vf.copy()\n",
    "print(\"Taille de table_vf_uni :\")\n",
    "print(table_vf_uni.shape)\n",
    "\n",
    "# On récupère les indices des transactions dupliquées...\n",
    "# i.e. les lignes pour lesquelles il y a les même identifiant de transaction\n",
    "dup_id = table_vf_dup.groupby('identifiant_transaction').size()\n",
    "dup_id = dup_id[dup_id > 1]\n",
    "dup_id = dup_id.reset_index()\n",
    "print(\"Voici à quoi ressemble dup_id :\")\n",
    "display(dup_id.sample(3))\n",
    "\n",
    "table_vf_dup = table_vf_dup[table_vf_dup['identifiant_transaction'].isin(dup_id[\"identifiant_transaction\"])]\n",
    "print(\"Voici à quoi ressemble table_vf_dup :\")\n",
    "display(table_vf_dup.head())\n",
    "print(\"Taille de table_vf_dup :\")\n",
    "display(table_vf_dup.shape)\n",
    "\n",
    "table_vf_uni = table_vf_uni[~table_vf_uni['identifiant_transaction'].isin(dup_id[\"identifiant_transaction\"])]\n",
    "print(\"Voici à quoi ressemble table_vf_uni :\")\n",
    "display(table_vf_uni.sample(3))\n",
    "print(\"Taille de table_vf_uni :\")\n",
    "display(table_vf_uni.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a8533a-b703-43eb-8a73-510b675f7a05",
   "metadata": {},
   "source": [
    "### 3.3. Rapport de mi-parcours pre-processing :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a603a5c-bb99-4771-96df-f377d5119092",
   "metadata": {},
   "source": [
    "Jusqu'ici, nous avons obtenu que :\n",
    "* le dataset initial (1 210 569 lignes) comporte **des erreurs qui nous empêchent de l'exploiter directement** ;\n",
    "* en effet, on distingue les transactions qui n'ont fait l'objet que d'**une seule ligne** (car il n'y avait pas de bien secondaire) des transactions qui font l'objet de **plusieurs lignes** (car il y a un ou plusieurs biens secondaires) : au total il n'y a eu que **654 843 transactions** (soit environ 1,85 lignes par transaction en moyenne).\n",
    "\n",
    "Nous avons ainsi créé **deux datasets** :\n",
    "* un dataset (**table_vf_uni**) avec uniquement les transactions n'ayant fait l'objet que d'une seule ligne (387 469 lignes) ;\n",
    "* un dataset (**table_vf_dup**) avec uniquement les transactions ayant fait l'objet de plusieurs lignes (823 100 lignes).\n",
    "\n",
    "Ainsi, nous avons désormais **deux options** :\n",
    "* nous pouvons *entraîner notre algorithme de pricing sur les transactions n'ayant fait l'objet que d'une seule ligne* (cela exclut par exemple les appartements avec une cave, les maisons avec un jardin d'agrément, etc.) -> cela représente tout de même près de 400 000 transactions en 2021 (60 % des transactions environ).\n",
    "* nous *continuons notre travail de preprocessing pour traiter le dataset table_vf_dup* avec les transactions dupliquées (40 % des transactions environ).\n",
    "\n",
    "Comme le traitement du dataset des transactions dupliquées est compliqué, nous avons choisi d'**étudier les deux possibilités** :\n",
    "* d'abord il conviendra de vérifier que les transactions ayant fait l'objet d'une ligne unique sont **représentatives du marché immobilier français** avant d'entraîner notre algorithme avec,\n",
    "* ensuite, nous pourrons **traiter le dataset des transactions dupliquées** et réentraîner notre algorithme avec ces nouvelles données.\n",
    "* nous pourrons enfin **comparer les performances des deux modèles**.\n",
    "\n",
    "Finalement, par manque de temps, nous n'entraînerons l'algorithme que sur les données uniques. Cependant, voici ci-dessous une méthode pour nettoyer les données dupliquées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f8cdc9-b792-4dda-9b4f-7f53bb53fa81",
   "metadata": {},
   "source": [
    "### 3.4. Traitement du dataset de transactions dupliquées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd34098-32aa-48a9-a82b-796da0627a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des colonnes principales d'intérêt pour le preprocessing :\n",
    "\n",
    "print(\"* Différentes valeurs dans 'type_local' :\")\n",
    "display([i for i in table_vf_dup['type_local'].unique()])\n",
    "\n",
    "print(\"* Différentes valeurs dans 'nature_culture' :\")\n",
    "display([i for i in table_vf_dup['nature_culture'].unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a9de5c-cc6e-4a87-957c-98be905ea7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objectif : créer des colonnes pour stocker les valeurs des biens secondaires (jardin, terrain à bâtir, dépendance, etc.)\n",
    "# On veut le nombre de m2 de la culture (s'il ne s'agit pas d'un jardin / terrain d'agrément ou d'un terrain à bâtir), \n",
    "# On veut aussi le nombre de m2 du local (une dépendance ou un local industriel, par exemple), du terrain à bâtir et du jardin\n",
    "\n",
    "table_vf_dup[\"culture_m2\"] = 0\n",
    "table_vf_dup[\"jardin_m2\"] = 0\n",
    "table_vf_dup[\"terrains_a_bâtir_m2\"] = 0\n",
    "table_vf_dup[\"dependance_m2\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891701ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une fonction de stockage des valeurs diffuses (faudra peut-être préciser ce qu'on veut dire par là) :\n",
    "\n",
    "def stockage_function(table_vf, table, indexes : list, saved_line_index):\n",
    "    \n",
    "    # On se place sur chaque \"ligne supplémentaire\"\n",
    "    for index in indexes:\n",
    "        \n",
    "        # On regarde d'abord si la nature du bien à la ligne index est la même que celle de la ligne de référence retenue\n",
    "        # S'il s'agit de la même nature de bien, c'est qu'on a eu une duplication (\"le fichier de restitution comportera (n x p) lignes\")\n",
    "        if table.loc[index, \"type_local\"] == table.loc[saved_line_index, \"type_local\"] and table.loc[index, \"type_local\"] != \"Nan\":\n",
    "            return table_vf\n",
    "        \n",
    "        # Sinon, on continue l'exploration...\n",
    "        else:\n",
    "            # On regarde s'il s'agit d'une dépendance ou d'un local industriel ou commercial\n",
    "            if table.loc[index, \"type_local\"] == \"Dépendance\":\n",
    "                if table_vf[\"surface_reelle_bati\"][index] != \"NaN\":\n",
    "                    table_vf[\"dependance_m2\"][saved_line_index] += table_vf[\"surface_reelle_bati\"][index]\n",
    "                if table_vf[\"surface_terrain\"][index] != \"NaN\":\n",
    "                    table_vf[\"dependance_m2\"][saved_line_index] += table_vf[\"surface_terrain\"][index]\n",
    "            elif table_vf_dup.loc[index, \"type_local\"] == \"Local industriel. commercial ou assimilé\":\n",
    "                if table_vf[\"surface_reelle_bati\"][index] != \"NaN\":\n",
    "                    table_vf[\"dependance_m2\"][saved_line_index] += table_vf[\"surface_reelle_bati\"][index]\n",
    "                if table_vf[\"surface_terrain\"][index] != \"NaN\":\n",
    "                    table_vf[\"dependance_m2\"][saved_line_index] += table_vf[\"surface_terrain\"][index]\n",
    "                    \n",
    "            # S'il ne s'agit pas d'une dépendance ou d'un local industriel ou commercial, alors il peut s'agir :\n",
    "            elif table.loc[index, \"type_local\"] == \"NaN\":\n",
    "                # Dans le cas où ça augmente la surface du terrain\n",
    "                if table_vf[\"surface_terrain\"][index] != \"NaN\":\n",
    "                    # D'un jardin\n",
    "                    if table_vf[\"nature_culture\"][index] == \"terrains d'agrément\":\n",
    "                        table_vf[\"jardin_m2\"][saved_line_index] += table_vf[\"surface_terrain\"][index]\n",
    "                    if table_vf[\"nature_culture\"][index] == \"jardins\":\n",
    "                        table_vf[\"jardin_m2\"][saved_line_index] += table_vf[\"surface_terrain\"][index]\n",
    "                    # D'un terrain à bâtir\n",
    "                    if table_vf[\"nature_culture\"][index] == 'sols':\n",
    "                        table_vf[\"terrains_a_bâtir_m2\"][saved_line_index] += table_vf[\"surface_terrain\"][index]\n",
    "                    if table_vf[\"nature_culture\"][index] == 'terrains a bâtir':\n",
    "                        table_vf[\"terrains_a_bâtir_m2\"][saved_line_index] += table_vf[\"surface_terrain\"][index]\n",
    "                    # D'une culture\n",
    "                    if table_vf[\"nature_culture\"][index] == 'culture':\n",
    "                        table_vf[\"culture_m2\"][saved_line_index] += table_vf[\"surface_terrain\"][index]\n",
    "                # Dans le cas où ça augmente la surface réelle du bâtiment\n",
    "                elif table_vf[\"surface_reelle_bati\"][index] != \"NaN\":\n",
    "                    if table_vf[\"nature_culture\"][index] == 'NaN':\n",
    "                        table_vf[\"surface_reelle_bati\"][saved_line_index] += table_vf[\"surface_reelle_bati\"][index]\n",
    "                    if table_vf[\"nature_culture\"][index] == 'sols':\n",
    "                        table_vf[\"surface_reelle_bati\"][saved_line_index] += table_vf[\"surface_reelle_bati\"][index]\n",
    "                else:\n",
    "                    return table_vf\n",
    "                \n",
    "        return table_vf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc0e0e-1c68-40a5-afad-2c6b14471999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ici, on montre à titre d'exemple pour les 500 premières lignes de table_vf_dup...\n",
    "# ... car la fonction met du temps à s'exécuter !\n",
    "# Donc, pour exploiter tout le dataset, il faudra supprimer cette cellule.\n",
    "\n",
    "table_vf_dup = table_vf_dup.copy()[:500]\n",
    "display(\"Taille initiale du dataset :\")\n",
    "display(table_vf_dup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83548769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objectif : stocker les valeurs des biens secondaires (jardin, terrain à bâtir, dépendance, etc.) dans les colonnes créées\n",
    "\n",
    "# On parcourt tous les identifiants de transactions uniques\n",
    "# On a importé tqdm pour visualiser la progression \n",
    "\n",
    "for identifiant in tqdm(table_vf_dup[\"identifiant_transaction\"].unique(), desc = \"Progression\"):\n",
    "    \n",
    "    # On crée une liste d'indices pour chaque identifiant de transaction unique\n",
    "    indexes = list(table_vf_dup[table_vf_dup[\"identifiant_transaction\"] == identifiant].index.values)\n",
    "        \n",
    "    # L'objectif est de trouver l'indice de référence : celui d'une maison, d'un appartement, etc.\n",
    "    # ... et lui ajouter des informations sur les dépendances / les cultures / les locaux\n",
    "    saved_line_index = indexes[0]\n",
    "    for index in indexes:\n",
    "        if table_vf_dup[\"type_local\"][index] == \"Maison\":\n",
    "            saved_line_index = index\n",
    "        elif table_vf_dup[\"type_local\"][index] == \"Appartement\":\n",
    "            saved_line_index = index\n",
    "        \n",
    "    # On prépare la liste des indices à enlever\n",
    "    removed_lines_indexes = []\n",
    "        \n",
    "    # On enlève de indexes l'indice de la ligne de référence\n",
    "    indexes.remove(saved_line_index)\n",
    "        \n",
    "    # On utilise la fonction de stockage\n",
    "    table_vf_dup = stockage_function(table_vf_dup, table, indexes, saved_line_index)\n",
    "    removed_lines_indexes.extend(indexes)\n",
    "    \n",
    "    # On enlève les lignes désormais inutiles dont les indices ont été stockés dans removed_lines_indexes\n",
    "    if len(removed_lines_indexes) != 0:\n",
    "        table_vf_dup.drop(removed_lines_indexes, 0, inplace = True)\n",
    "\n",
    "display(\"Taille finale du dataset :\")\n",
    "display(table_vf_dup.shape)\n",
    "display(table_vf_dup.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f3a41b-2120-4687-943d-b938d2c6db12",
   "metadata": {},
   "source": [
    "### 3.5. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717080d3-2c24-48f4-80a6-f0f563dad54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On observe que le DataFrame est désormais exploitable :\n",
    "\n",
    "display(\"Observation du DataFrame :\")\n",
    "display(table_vf_dup.head(5))\n",
    "\n",
    "display(\"Nombre d'identifiant_transaction uniques dans le DataFrame :\")\n",
    "display(len(table_vf_dup[\"identifiant_transaction\"].unique()))\n",
    "\n",
    "display(\"Nombre de lignes dans le DataFrame :\")\n",
    "display(len(table_vf_dup))\n",
    "\n",
    "display(\"Nombre moyen de lignes par vente :\")\n",
    "np.round(len(table_vf_dup) / len(table_vf_dup[\"identifiant_transaction\"].unique()), 2)\n",
    "\n",
    "# Attention, lorsque la surface d'une dépendance n'est pas renseignée, il est donné NaN par défaut... \n",
    "# Comment l'algorithme interprétera-t-il ce NaN ? Il s'agit de lui donner une valeur !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d18ca91-3722-4a88-8a54-53445c17b68e",
   "metadata": {},
   "source": [
    "Il ne resterait plus qu'à **merger les deux dataframes** pour obtenir toutes les informations de l'année étudiée.\n",
    "\n",
    "Comme nous l'avons déjà dit, le code de nettoyage est très long à exécuter pour le dataset entier du S1 2021 (12h estimées par tqdm). Nous nous sommes donc **concentrés sur le dataset des transactions avec une seule ligne** pour l'entraînement du modèle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
