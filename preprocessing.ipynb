{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e8296a2-130c-412f-b470-49c5ae1532b4",
   "metadata": {},
   "source": [
    "# Etape 1 : preprocessing DVF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c80171-df05-43a6-b027-1ebabfd7a1a6",
   "metadata": {},
   "source": [
    "### 1.1. Importation du dataset DVF :\n",
    "\n",
    "Nous importons le dataset « **Demandes de valeurs foncières** » (DVF), publié par la DGFiP, permet de connaître les transactions immobilières intervenues au cours des cinq dernières années sur le territoire métropolitain et les DOM-TOM, à l’exception de l’Alsace, de la Moselle et de Mayotte. Les données contenues sont issues des actes notariés et des informations cadastrales.\n",
    "\n",
    "Fichiers 2017-2020 : https://files.data.gouv.fr/geo-dvf/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf301ebd-3b13-4f6e-b1f0-7f743874aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"https://files.data.gouv.fr/geo-dvf/latest/csv/2021/full.csv.gz\"\n",
    "\n",
    "def load_data():\n",
    "    data = pd.read_csv(name, sep = ',')\n",
    "    \"\"\"\n",
    "    for year in range(2017, 2021):\n",
    "        name = \"https://files.data.gouv.fr/geo-dvf/latest/csv/\" + str(year) + \"/full.csv.gz\"\n",
    "        table = pd.concat([table, pd.read_csv(name, sep = ',')])\n",
    "\n",
    "    display(\"Taille de table :\")\n",
    "    display(table.shape)\n",
    "    table.head()\n",
    "    \"\"\"\n",
    "    return data\n",
    "\n",
    "table = load_data()\n",
    "\n",
    "# On ne travaille que sur les données du S1 2021, afin d'avoir des calculs moins coûteux en temps.\n",
    "# Les transactions du S1 2021 représentent tout de même un dataset de 1 200 000 lignes...\n",
    "# Pour travailler sur l'ensemble des données (2017-2021), il suffit d'enlever les guillemets ci-dessus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3f9f02-ad7c-4214-9690-57750c6e8481",
   "metadata": {},
   "source": [
    "### 1.2. Visualisation des données DVF\n",
    "\n",
    "Il s'agit dans cette section de se faire **des premières intuitions sur les données**. \n",
    "\n",
    "En particulier, on se rend compte d'un **problème de preprocessing** : une transaction peut correspondre à plusieurs lignes avec la même valeur foncière sur chaque ligne. Autrement dit, **DVF affiche le même prix de vente global à chaque lot d'une même transaction**.\n",
    "\n",
    "Pour observer cela, nous allons créer **un identifiant de transaction unique**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b49e2d4-1702-4306-b526-64439d54d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une adresse générique\n",
    "\n",
    "table['adresse_numero'] = table['adresse_numero'].fillna('0').astype(int)\n",
    "table['adresse_suffixe'] = table['adresse_suffixe'].fillna(' ')\n",
    "table['adresse_code_voie'] = table['adresse_code_voie'].fillna(' ')\n",
    "table['adresse_nom_voie'] = table['adresse_nom_voie'].fillna(' ')\n",
    "table['code_postal'] = table['code_postal'].fillna('0').astype(int)\n",
    "table['nom_commune'] = table['nom_commune'].fillna(' ')\n",
    "\n",
    "# Ajout de \"\\\" pour que l'opération soit visible à l'écran en entier\n",
    "table[\"adresse\"] = table['adresse_numero'].astype(str) + ' ' + table['adresse_suffixe'] + ' ' + \\\n",
    "                table['adresse_code_voie'] + ' ' + table['adresse_nom_voie'] + ' ' + table['nom_commune'] + ' ' + \\\n",
    "                table['code_postal'].astype(str) + ' ' + 'France'\n",
    "\n",
    "# Création d'un identifiant de transaction\n",
    "\n",
    "# Pour identifier les doublons, l'adresse ne suffit pas : un bien peut avoir été vendu deux fois dans la même année\n",
    "table[\"identifiant_transaction\"] = table[\"adresse\"].astype(str) + ' le ' + table[\"date_mutation\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b7e85-f328-4edd-919a-a8216d01cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problème dans les données et vérification de la validité de l'identifiant de transaction :\n",
    "\n",
    "display(table[\"identifiant_transaction\"].loc[0])\n",
    "display(table[\"identifiant_transaction\"].loc[1])\n",
    "display(\"Si l'identifiant de transaction est valide, alors True doit s'afficher :\")\n",
    "table[\"identifiant_transaction\"].loc[0] == table[\"identifiant_transaction\"].loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8016e7-e85b-4487-bdc9-98f7b3fafa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On constate, encore une fois, le problème relevé ci-dessus :\n",
    "\n",
    "display(\"Nombre d'adresses uniques dans le DataFrame :\")\n",
    "display(len(table[\"adresse\"].unique()))\n",
    "\n",
    "display(\"Nombre d'identifiant_transaction uniques dans le DataFrame :\")\n",
    "display(len(table[\"identifiant_transaction\"].unique()))\n",
    "\n",
    "display(\"Nombre de lignes dans le DataFrame :\")\n",
    "display(len(table))\n",
    "\n",
    "display(\"Nombre moyen de lignes par vente :\")\n",
    "np.round(len(table) / len(table[\"identifiant_transaction\"].unique()), 2)\n",
    "\n",
    "# Une vente correspond à plusieurs lignes, les informations sont donc diffusées dans ces lignes..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be97366e-94aa-42ae-9f33-2abc109442f5",
   "metadata": {},
   "source": [
    "Dès lors, si on entraîne l'algorithme de pricing sur ce dataset, il sera **biaisé** : \n",
    "* d'une part, il associerait à une dépendance de 20 m2 le prix d'un appartement de 200 m2\n",
    "* d'autre part, il ne prendrait pas en compte la plus-value apportée par un jardin à une maison, par une dépendance à un appartement, etc.\n",
    "\n",
    "Il conviendra donc de **retravailler les données pour obtenir une seule ligne par transaction**.\n",
    "\n",
    "**Le notebook \"cleaning-dvf\"** (lien : https://github.com/victor-kerros/tokenisation-immo/blob/main/cleaning-dvf.ipynb) revient en détail sur ce problème de preprocessing et effectue ce travail de nettoyage. Dans la mesure où cette solution de nettoyage est très coûteuse en temps, nous privilégions **une solution plus efficace** dans le cadre de ce projet : nous ne retenons que les transactions ne faisant l'objet que d'une seule ligne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e04503-143b-4e58-aac1-90674fe54cbe",
   "metadata": {},
   "source": [
    "### 1.3. Création du dataset final DVF\n",
    "\n",
    "D'abord, nous ne prenons que les colonnes suivantes :\n",
    "- Date de vente/mutation\n",
    "- Nature mutation (pour séparer les ventes en VEFA et les ventes classiques)\n",
    "- Valeur foncière (prix de vente)\n",
    "- Colonnes liées à l’adresse (pour nous permettre de localiser le bien)\n",
    "- Adresse\n",
    "- Code Postal\n",
    "- Type local (maison/appartement/Local commercial/Dépendance etc)\n",
    "- Surface réelle bâtie (nb de mètre carré du bien bâti)\n",
    "- Surface terrain (nb de mètre carré du terrain associé au bien)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563c9574-71e1-4cd7-a4ea-e732de2bf17c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_734/3927942465.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0;34m'surface_reelle_bati'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nombre_pieces_principales'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nature_culture'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'surface_terrain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'longitude'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             'latitude', 'adresse', 'code_departement', 'identifiant_transaction']\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtable_vf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolonnes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# On agrège les types de cultures différents de NaN, sols, terrain à bâtir et  : on les renomme \"culture\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'table' is not defined"
     ]
    }
   ],
   "source": [
    "# On crée le dataframe table_vf avec les seules colonnes qui nous intéressent\n",
    "colonnes = [\"date_mutation\", \"nature_mutation\", \"valeur_fonciere\", \"code_postal\", 'type_local',\n",
    "            'surface_reelle_bati', 'nombre_pieces_principales', 'nature_culture', 'surface_terrain', 'longitude', \n",
    "            'latitude', 'adresse', 'code_departement', 'identifiant_transaction']\n",
    "table_vf = table[colonnes].copy()\n",
    "\n",
    "# On agrège les types de cultures différents de NaN, sols, terrain à bâtir et  : on les renomme \"culture\"\n",
    "culture_type = ['taillis simples', 'eaux', 'landes', 'taillis sous futaie', 'prés', 'terres', 'peupleraies', \n",
    "                'vignes', 'bois', 'vergers', 'carrières', 'futaies résineuses', 'pâtures', 'futaies feuillues', \n",
    "                'futaies mixtes', 'chemin de fer', 'oseraies', 'pacages', 'prés plantes', 'terres plantées', \n",
    "                'landes boisées', 'herbages', \"prés d'embouche\"]\n",
    "\n",
    "for x in culture_type:\n",
    "    table_vf.loc[table_vf[\"nature_culture\"] == x, \"nature_culture\"] = \"culture\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42a958f-e491-4d07-bddf-f98022965a02",
   "metadata": {},
   "source": [
    "Ensuite, comme annoncé, **nous ne retenons que les transactions ne faisant l'objet que d'une seule ligne pour créer \"data\"** : le dataset final avec lequel nous allons travailler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233d5da9-1c18-4076-93ae-e2d70e237f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ne retient que les transactions ayant fait l'objet d'une seule et unique ligne\n",
    "table_vf_dup = table_vf.copy()\n",
    "table_vf_uni = table_vf.copy()\n",
    "\n",
    "# On récupère les indices des transactions dupliquées...\n",
    "# i.e. les lignes où on retrouve un id de transaction utilisé ailleurs\n",
    "dup_id = table_vf_dup.groupby('identifiant_transaction').size()\n",
    "dup_id = dup_id[dup_id > 1]\n",
    "dup_id = dup_id.reset_index()\n",
    "\n",
    "table_vf_dup = table_vf_dup[table_vf_dup['identifiant_transaction'].isin(dup_id[\"identifiant_transaction\"])]\n",
    "table_vf_uni = table_vf_uni[~table_vf_uni['identifiant_transaction'].isin(dup_id[\"identifiant_transaction\"])]\n",
    "\n",
    "print(\"Taille de table_vf_uni (nombre de transactions non dupliquées) :\")\n",
    "print(table_vf_uni.shape)\n",
    "\n",
    "# La solution la plus efficace, pour éviter un nettoyage trop coûteux en temps...\n",
    "# est de ne retenir que les transactions non dupliquées.\n",
    "data = table_vf_uni\n",
    "data = data.reset_index().drop(\"index\", axis = 1)\n",
    "\n",
    "# Visualisation de data\n",
    "display(\"Les trois premières lignes de data :\")\n",
    "display(data.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c86018-de66-4ffb-8852-bc841944405a",
   "metadata": {},
   "source": [
    "### 1.4. Valeurs extrêmes dans le dataset DVF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c25d66-a7a5-455b-b7c4-4c235772eb61",
   "metadata": {},
   "source": [
    "On s'intéresse aux **valeurs extrêmes dans le dataset**. En effet, afin d'avoir un entraînement fiable, nous devons les enlever (sinon, il y aura trop de bruit)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
